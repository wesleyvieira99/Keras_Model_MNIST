{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "tensor-flow-2-1",
      "graded_item_id": "g0YqY",
      "launcher_item_id": "N6gmY"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-21wiLf-gCD"
      },
      "source": [
        "# Programming Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxkainBa-gCF"
      },
      "source": [
        "## CNN classifier for the MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQKECTiE-gCG"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In this notebook, you will write code to build, compile and fit a convolutional neural network (CNN) model to the MNIST dataset of images of handwritten digits.\n",
        "\n",
        "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line:\n",
        "\n",
        "`#### GRADED CELL ####`\n",
        "\n",
        "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly. Inside these graded cells, you can use any functions or classes that are imported below, but make sure you don't use any variables that are outside the scope of the function.\n",
        "\n",
        "### How to submit\n",
        "\n",
        "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the **Submit Assignment** button at the top of this notebook.\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR7qaZCl-gCJ"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# If you would like to make further imports from Tensorflow, add them here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOQk31Sc-gCN"
      },
      "source": [
        "#### The MNIST dataset\n",
        "\n",
        "In this assignment, you will use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). It consists of a training set of 60,000 handwritten digits with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The dataset is frequently used in machine learning research, and has become a standard benchmark for image classification models.\n",
        "\n",
        "- Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. \"Gradient-based learning applied to document recognition.\" Proceedings of the IEEE, 86(11):2278-2324, November 1998.\n",
        "\n",
        "Your goal is to construct a neural network that classifies images of handwritten digits into one of 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOxMGi5e-gCP"
      },
      "source": [
        "#### Load and preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zzRQzxA-gCQ"
      },
      "source": [
        "# Run this cell to load the MNIST data\n",
        "\n",
        "mnist_data = tf.keras.datasets.mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist_data.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEeA_9-6-gCV"
      },
      "source": [
        "First, preprocess the data by scaling the training and test images so their values lie in the range from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AW1YX_9-gCX"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def scale_mnist_data(train_images, test_images):\n",
        "    # Scale training images\n",
        "    train_images = train_images / 255.0\n",
        "\n",
        "    # Scale test images\n",
        "    test_images = test_images / 255.0\n",
        "\n",
        "    return train_images, test_images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgMBPB9d-gCa"
      },
      "source": [
        "# Run your function on the input data\n",
        "\n",
        "scaled_train_images, scaled_test_images = scale_mnist_data(train_images, test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1r-ULOQv2o3"
      },
      "source": [
        "# Add a dummy channel dimension\n",
        "\n",
        "scaled_train_images = scaled_train_images[..., np.newaxis]\n",
        "scaled_test_images = scaled_test_images[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy--eSWq-gCc"
      },
      "source": [
        "#### Build the convolutional neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rnippry-gCd"
      },
      "source": [
        "We are now ready to construct a model to fit to the data. Using the Sequential API, build your CNN model according to the following spec:\n",
        "\n",
        "* The model should use the `input_shape` in the function argument to set the input size in the first layer.\n",
        "* A 2D convolutional layer with a 3x3 kernel and 8 filters. Use 'SAME' zero padding and ReLU activation functions. Make sure to provide the `input_shape` keyword argument in this first layer.\n",
        "* A max pooling layer, with a 2x2 window, and default strides.\n",
        "* A flatten layer, which unrolls the input into a one-dimensional tensor.\n",
        "* Two dense hidden layers, each with 64 units and ReLU activation functions.\n",
        "* A dense output layer with 10 units and the softmax activation function.\n",
        "\n",
        "In particular, your neural network should have six layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-N7ArQ1-gCe"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def get_model(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(8, (3, 3), padding='same', activation='relu', input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9L_2kj9A-gCi"
      },
      "source": [
        "# Run your function to get the model\n",
        "\n",
        "model = get_model(scaled_train_images[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvrW1EA1-gCl"
      },
      "source": [
        "#### Compile the model\n",
        "\n",
        "You should now compile the model using the `compile` method. To do so, you need to specify an optimizer, a loss function and a metric to judge the performance of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_x9mU2Li-gCm"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def compile_model(model):\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY08R9yB-gCr"
      },
      "source": [
        "# Run your function to compile the model\n",
        "\n",
        "compile_model(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHUcXibk-gCv"
      },
      "source": [
        "#### Fit the model to the training data\n",
        "\n",
        "Now you should train the model on the MNIST dataset, using the model's `fit` method. Set the training to run for 5 epochs, and return the training history to be used for plotting the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDnNXqN1-gCw"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "\n",
        "def train_model(model, scaled_train_images, train_labels):\n",
        "    history = model.fit(scaled_train_images, train_labels, epochs=5)\n",
        "    return history\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1n3wh49-gCz"
      },
      "source": [
        "# Run your function to train the model\n",
        "\n",
        "history = train_model(model, scaled_train_images, train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhd3yK0i-gC3"
      },
      "source": [
        "#### Plot the learning curves\n",
        "\n",
        "We will now plot two graphs:\n",
        "* Epoch vs accuracy\n",
        "* Epoch vs loss\n",
        "\n",
        "We will load the model history into a pandas `DataFrame` and use the `plot` method to output the required graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0t2Xjgq-gC4"
      },
      "source": [
        "# Run this cell to load the model history into a pandas DataFrame\n",
        "\n",
        "frame = pd.DataFrame(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQqYQiR4-gC7"
      },
      "source": [
        "# Run this cell to make the Accuracy vs Epochs plot\n",
        "\n",
        "acc_plot = frame.plot(y=\"accuracy\", title=\"Accuracy vs Epochs\", legend=False)\n",
        "acc_plot.set(xlabel=\"Epochs\", ylabel=\"Accuracy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGgTGfH4-gDA"
      },
      "source": [
        "# Run this cell to make the Loss vs Epochs plot\n",
        "\n",
        "acc_plot = frame.plot(y=\"loss\", title = \"Loss vs Epochs\",legend=False)\n",
        "acc_plot.set(xlabel=\"Epochs\", ylabel=\"Loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziq-tFlU-gDD"
      },
      "source": [
        "#### Evaluate the model\n",
        "\n",
        "Finally, you should evaluate the performance of your model on the test set, by calling the model's `evaluate` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSqA8zUi-gDE"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function.\n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def evaluate_model(model, scaled_test_images, test_labels):\n",
        "    test_loss, test_accuracy = model.evaluate(scaled_test_images, test_labels)\n",
        "    return test_loss, test_accuracy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSNhInQD-gDG"
      },
      "source": [
        "# Run your function to evaluate the model\n",
        "\n",
        "test_loss, test_accuracy = evaluate_model(model, scaled_test_images, test_labels)\n",
        "print(f\"Test loss: {test_loss}\")\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP09yVMK-gDK"
      },
      "source": [
        "#### Model predictions\n",
        "\n",
        "Let's see some model predictions! We will randomly select four images from the test data, and display the image and label for each.\n",
        "\n",
        "For each test image, model's prediction (the label with maximum probability) is shown, together with a plot showing the model's categorical distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrUM42t_-gDL"
      },
      "source": [
        "# Run this cell to get model predictions on randomly selected test images\n",
        "\n",
        "num_test_images = scaled_test_images.shape[0]\n",
        "\n",
        "random_inx = np.random.choice(num_test_images, 4)\n",
        "random_test_images = scaled_test_images[random_inx, ...]\n",
        "random_test_labels = test_labels[random_inx, ...]\n",
        "\n",
        "predictions = model.predict(random_test_images)\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
        "fig.subplots_adjust(hspace=0.4, wspace=-0.2)\n",
        "\n",
        "for i, (prediction, image, label) in enumerate(zip(predictions, random_test_images, random_test_labels)):\n",
        "    axes[i, 0].imshow(np.squeeze(image))\n",
        "    axes[i, 0].get_xaxis().set_visible(False)\n",
        "    axes[i, 0].get_yaxis().set_visible(False)\n",
        "    axes[i, 0].text(10., -1.5, f'Digit {label}')\n",
        "    axes[i, 1].bar(np.arange(len(prediction)), prediction)\n",
        "    axes[i, 1].set_xticks(np.arange(len(prediction)))\n",
        "    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {np.argmax(prediction)}\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y6mwJLs-gDP"
      },
      "source": [
        "Congratulations for completing this programming assignment! In the next week of the course we will take a look at including validation and regularisation in our model training, and introduce Keras callbacks."
      ]
    }
  ]
}